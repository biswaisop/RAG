{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd64e6b8",
   "metadata": {},
   "source": [
    "## Intro to Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8c65ccf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9252c05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    CharacterTextSplitter,\n",
    "    TokenTextSplitter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3382f4a5",
   "metadata": {},
   "source": [
    "## Understanding te document structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "758fe797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Structure\n",
      "\n",
      "\n",
      "content: This is the main text content that will be embeded and searched\n",
      "metadata: {'source': 'example.txt', 'page': 1, 'author': 'Biswa', 'date_created': '31-08-2024', 'custom_field': 'any value'}\n"
     ]
    }
   ],
   "source": [
    "## create a simple document\n",
    "doc = Document(\n",
    "    page_content=\"This is the main text content that will be embeded and searched\",\n",
    "    metadata = {\n",
    "        \"source\":\"example.txt\",\n",
    "        \"page\":1,\n",
    "        \"author\":\"Biswa\",\n",
    "        \"date_created\":\"31-08-2024\",\n",
    "        \"custom_field\":\"any value\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Document Structure\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"content: {doc.page_content}\")\n",
    "print(f\"metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadcfbc5",
   "metadata": {},
   "source": [
    "## Text files - the simplest use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0b466f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"data/text_files\", exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "94bb8689",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = {\n",
    "    \"data/text_files/python_intro.txt\": \"\"\"\\\n",
    "# Introduction to Python\n",
    "\n",
    "Python is one of the most popular programming languages used for Data Science, Web Development, and AI.\n",
    "In this notebook (1-data_ingestion.ipynb), we will cover:\n",
    "\n",
    "- Setting up Python environment\n",
    "- Understanding variables and data types\n",
    "- Writing simple functions\n",
    "- Reading and writing files\n",
    "- A quick intro to using external libraries\n",
    "\n",
    "By the end, you will be able to write basic Python programs and prepare for more advanced topics.\n",
    "\"\"\",\n",
    "\n",
    "    \"data/text_files/data_cleaning.txt\": \"\"\"\\\n",
    "# Data Cleaning\n",
    "\n",
    "Data cleaning is one of the most important steps before applying any ML model.\n",
    "In this notebook (2-data_cleaning.ipynb), we will cover:\n",
    "\n",
    "- Handling missing values (drop, fill, interpolate)\n",
    "- Removing duplicates\n",
    "- Fixing inconsistent formatting\n",
    "- Detecting and removing outliers\n",
    "- Standardizing column names\n",
    "\n",
    "Clean data = Better results for your ML pipeline.\n",
    "\"\"\",\n",
    "\n",
    "    \"data/text_files/eda.txt\": \"\"\"\\\n",
    "# Exploratory Data Analysis (EDA)\n",
    "\n",
    "EDA helps us understand the data better before modeling.\n",
    "In this notebook (3-eda.ipynb), we will:\n",
    "\n",
    "- Summarize datasets with Pandas\n",
    "- Visualize distributions (histograms, boxplots, scatterplots)\n",
    "- Correlation analysis\n",
    "- Feature relationships\n",
    "\n",
    "Good EDA often reveals hidden patterns in the data.\n",
    "\"\"\",\n",
    "\n",
    "    \"data/text_files/feature_engineering.txt\": \"\"\"\\\n",
    "# Feature Engineering\n",
    "\n",
    "Feature Engineering is the art of creating better inputs for ML models.\n",
    "In this notebook (4-feature_engineering.ipynb), we will:\n",
    "\n",
    "- Encoding categorical variables (OneHot, Label Encoding)\n",
    "- Normalization and Standardization\n",
    "- Binning numerical variables\n",
    "- Feature selection techniques\n",
    "- Creating interaction features\n",
    "\n",
    "Better features = Smarter models.\n",
    "\"\"\",\n",
    "\n",
    "    \"data/text_files/model_training.txt\": \"\"\"\\\n",
    "# Model Training\n",
    "\n",
    "In this notebook (5-model_training.ipynb), we will train our ML models.\n",
    "Topics covered:\n",
    "\n",
    "- Train/Test split\n",
    "- Cross-validation\n",
    "- Training models (Linear Regression, Decision Trees, Random Forests)\n",
    "- Hyperparameter tuning (GridSearchCV, RandomizedSearchCV)\n",
    "- Saving and loading models with joblib/pickle\n",
    "\"\"\",\n",
    "\n",
    "    \"data/text_files/model_evaluation.txt\": \"\"\"\\\n",
    "# Model Evaluation\n",
    "\n",
    "The last step in the ML pipeline is model evaluation.\n",
    "In this notebook (6-model_evaluation.ipynb), we will cover:\n",
    "\n",
    "- Accuracy, Precision, Recall, F1-score\n",
    "- ROC-AUC curves\n",
    "- Confusion Matrix\n",
    "- Bias-variance tradeoff\n",
    "- Model interpretability with SHAP/feature importance\n",
    "\n",
    "Evaluation ensures that our model is reliable and ready for deployment.\n",
    "\"\"\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6f919ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created sample files succesfully\n"
     ]
    }
   ],
   "source": [
    "for filepath,content in sample_text.items():\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"Created sample files succesfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e90c10",
   "metadata": {},
   "source": [
    "### TextLoader - Read single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e3e866b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4707b29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(\"data/text_files/python_intro.txt\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d4f3f4ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/text_files/python_intro.txt'}, page_content='# Introduction to Python\\n\\nPython is one of the most popular programming languages used for Data Science, Web Development, and AI.\\nIn this notebook (1-data_ingestion.ipynb), we will cover:\\n\\n- Setting up Python environment\\n- Understanding variables and data types\\n- Writing simple functions\\n- Reading and writing files\\n- A quick intro to using external libraries\\n\\nBy the end, you will be able to write basic Python programs and prepare for more advanced topics.\\n')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = loader.load()\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ec95d727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(document))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f69142",
   "metadata": {},
   "source": [
    "### Directory loader - Reading multiple text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "586379eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "121d1d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 2001.10it/s]\n"
     ]
    }
   ],
   "source": [
    "dir_loader = DirectoryLoader(\n",
    "    \"data/text_files\",\n",
    "    glob = \"**/*.txt\", ## pattern to match files\n",
    "    loader_cls = TextLoader, ## loader class to use\n",
    "    loader_kwargs = {'encoding':'utf-8'},\n",
    "    show_progress = True\n",
    ")\n",
    "\n",
    "documents2 = dir_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0bb4baca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data\\\\text_files\\\\data_cleaning.txt'}, page_content='# Data Cleaning\\n\\nData cleaning is one of the most important steps before applying any ML model.\\nIn this notebook (2-data_cleaning.ipynb), we will cover:\\n\\n- Handling missing values (drop, fill, interpolate)\\n- Removing duplicates\\n- Fixing inconsistent formatting\\n- Detecting and removing outliers\\n- Standardizing column names\\n\\nClean data = Better results for your ML pipeline.\\n'),\n",
       " Document(metadata={'source': 'data\\\\text_files\\\\eda.txt'}, page_content='# Exploratory Data Analysis (EDA)\\n\\nEDA helps us understand the data better before modeling.\\nIn this notebook (3-eda.ipynb), we will:\\n\\n- Summarize datasets with Pandas\\n- Visualize distributions (histograms, boxplots, scatterplots)\\n- Correlation analysis\\n- Feature relationships\\n\\nGood EDA often reveals hidden patterns in the data.\\n'),\n",
       " Document(metadata={'source': 'data\\\\text_files\\\\feature_engineering.txt'}, page_content='# Feature Engineering\\n\\nFeature Engineering is the art of creating better inputs for ML models.\\nIn this notebook (4-feature_engineering.ipynb), we will:\\n\\n- Encoding categorical variables (OneHot, Label Encoding)\\n- Normalization and Standardization\\n- Binning numerical variables\\n- Feature selection techniques\\n- Creating interaction features\\n\\nBetter features = Smarter models.\\n'),\n",
       " Document(metadata={'source': 'data\\\\text_files\\\\model_evaluation.txt'}, page_content='# Model Evaluation\\n\\nThe last step in the ML pipeline is model evaluation.\\nIn this notebook (6-model_evaluation.ipynb), we will cover:\\n\\n- Accuracy, Precision, Recall, F1-score\\n- ROC-AUC curves\\n- Confusion Matrix\\n- Bias-variance tradeoff\\n- Model interpretability with SHAP/feature importance\\n\\nEvaluation ensures that our model is reliable and ready for deployment.\\n'),\n",
       " Document(metadata={'source': 'data\\\\text_files\\\\model_training.txt'}, page_content='# Model Training\\n\\nIn this notebook (5-model_training.ipynb), we will train our ML models.\\nTopics covered:\\n\\n- Train/Test split\\n- Cross-validation\\n- Training models (Linear Regression, Decision Trees, Random Forests)\\n- Hyperparameter tuning (GridSearchCV, RandomizedSearchCV)\\n- Saving and loading models with joblib/pickle\\n'),\n",
       " Document(metadata={'source': 'data\\\\text_files\\\\python_intro.txt'}, page_content='# Introduction to Python\\n\\nPython is one of the most popular programming languages used for Data Science, Web Development, and AI.\\nIn this notebook (1-data_ingestion.ipynb), we will cover:\\n\\n- Setting up Python environment\\n- Understanding variables and data types\\n- Writing simple functions\\n- Reading and writing files\\n- A quick intro to using external libraries\\n\\nBy the end, you will be able to write basic Python programs and prepare for more advanced topics.\\n')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "26c0c59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6\n",
      "\n",
      "Document 2\n",
      "source: data\\text_files\\data_cleaning.txt\n",
      "Length: 375 characters\n",
      "\n",
      "Document 2\n",
      "source: data\\text_files\\eda.txt\n",
      "Length: 330 characters\n",
      "\n",
      "Document 2\n",
      "source: data\\text_files\\feature_engineering.txt\n",
      "Length: 375 characters\n",
      "\n",
      "Document 2\n",
      "source: data\\text_files\\model_evaluation.txt\n",
      "Length: 363 characters\n",
      "\n",
      "Document 2\n",
      "source: data\\text_files\\model_training.txt\n",
      "Length: 321 characters\n",
      "\n",
      "Document 2\n",
      "source: data\\text_files\\python_intro.txt\n",
      "Length: 460 characters\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loaded {len(documents2)}\")\n",
    "for i, doc in enumerate(documents2):\n",
    "    print(f\"\\nDocument {1+1}\")\n",
    "    print(f\"source: {doc.metadata['source']}\")\n",
    "    print(f\"Length: {len(doc.page_content)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3acf437",
   "metadata": {},
   "source": [
    "## Text Splitting Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2ca6e9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'data\\\\text_files\\\\data_cleaning.txt'}, page_content='# Data Cleaning\\n\\nData cleaning is one of the most important steps before applying any ML model.\\nIn this notebook (2-data_cleaning.ipynb), we will cover:\\n\\n- Handling missing values (drop, fill, interpolate)\\n- Removing duplicates\\n- Fixing inconsistent formatting\\n- Detecting and removing outliers\\n- Standardizing column names\\n\\nClean data = Better results for your ML pipeline.\\n'), Document(metadata={'source': 'data\\\\text_files\\\\eda.txt'}, page_content='# Exploratory Data Analysis (EDA)\\n\\nEDA helps us understand the data better before modeling.\\nIn this notebook (3-eda.ipynb), we will:\\n\\n- Summarize datasets with Pandas\\n- Visualize distributions (histograms, boxplots, scatterplots)\\n- Correlation analysis\\n- Feature relationships\\n\\nGood EDA often reveals hidden patterns in the data.\\n'), Document(metadata={'source': 'data\\\\text_files\\\\feature_engineering.txt'}, page_content='# Feature Engineering\\n\\nFeature Engineering is the art of creating better inputs for ML models.\\nIn this notebook (4-feature_engineering.ipynb), we will:\\n\\n- Encoding categorical variables (OneHot, Label Encoding)\\n- Normalization and Standardization\\n- Binning numerical variables\\n- Feature selection techniques\\n- Creating interaction features\\n\\nBetter features = Smarter models.\\n'), Document(metadata={'source': 'data\\\\text_files\\\\model_evaluation.txt'}, page_content='# Model Evaluation\\n\\nThe last step in the ML pipeline is model evaluation.\\nIn this notebook (6-model_evaluation.ipynb), we will cover:\\n\\n- Accuracy, Precision, Recall, F1-score\\n- ROC-AUC curves\\n- Confusion Matrix\\n- Bias-variance tradeoff\\n- Model interpretability with SHAP/feature importance\\n\\nEvaluation ensures that our model is reliable and ready for deployment.\\n'), Document(metadata={'source': 'data\\\\text_files\\\\model_training.txt'}, page_content='# Model Training\\n\\nIn this notebook (5-model_training.ipynb), we will train our ML models.\\nTopics covered:\\n\\n- Train/Test split\\n- Cross-validation\\n- Training models (Linear Regression, Decision Trees, Random Forests)\\n- Hyperparameter tuning (GridSearchCV, RandomizedSearchCV)\\n- Saving and loading models with joblib/pickle\\n'), Document(metadata={'source': 'data\\\\text_files\\\\python_intro.txt'}, page_content='# Introduction to Python\\n\\nPython is one of the most popular programming languages used for Data Science, Web Development, and AI.\\nIn this notebook (1-data_ingestion.ipynb), we will cover:\\n\\n- Setting up Python environment\\n- Understanding variables and data types\\n- Writing simple functions\\n- Reading and writing files\\n- A quick intro to using external libraries\\n\\nBy the end, you will be able to write basic Python programs and prepare for more advanced topics.\\n')]\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import (\n",
    "    CharacterTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    TokenTextSplitter\n",
    ")\n",
    "\n",
    "print(documents2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ffc6a3",
   "metadata": {},
   "source": [
    "#### Character Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "30387f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Data Cleaning\\n\\nData cleaning is one of the most important steps before applying any ML model.\\nIn this notebook (2-data_cleaning.ipynb), we will cover:\\n\\n- Handling missing values (drop, fill, interpolate)\\n- Removing duplicates\\n- Fixing inconsistent formatting\\n- Detecting and removing outliers\\n- Standardizing column names\\n\\nClean data = Better results for your ML pipeline.\\n'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = documents2[0].page_content\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2e359aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size = 200,\n",
    "    chunk_overlap = 20,\n",
    "    length_function = len\n",
    ")\n",
    "\n",
    "char_chunks = char_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e26dfd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created 3 chunks\n",
      "first chunk # Data Cleaning\n",
      "Data cleaning is one of the most important steps before applying any ML model.\n",
      "In this notebook (2-data_cleaning.ipynb), we will cover:\n"
     ]
    }
   ],
   "source": [
    "print(f\"created {len(char_chunks)} chunks\")\n",
    "print(f\"first chunk {char_chunks[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "63f28ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "recur_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\",\"\\n\",\" \", \"\"],\n",
    "    chunk_size = 200,\n",
    "    chunk_overlap = 5,\n",
    "    length_function = len\n",
    ")\n",
    "\n",
    "recur_chunks = recur_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "59720216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created 3 chunks\n",
      "first chunk # Data Cleaning\n",
      "\n",
      "Data cleaning is one of the most important steps before applying any ML model.\n",
      "In this notebook (2-data_cleaning.ipynb), we will cover:\n",
      "\n",
      "second chunk - Handling missing values (drop, fill, interpolate)\n",
      "- Removing duplicates\n",
      "- Fixing inconsistent formatting\n",
      "- Detecting and removing outliers\n",
      "- Standardizing column names\n",
      "\n",
      "third chunk Clean data = Better results for your ML pipeline.\n"
     ]
    }
   ],
   "source": [
    "print(f\"created {len(recur_chunks)} chunks\")\n",
    "print(f\"first chunk {recur_chunks[0]}\")\n",
    "print()\n",
    "print(f\"second chunk {recur_chunks[1]}\")\n",
    "print()\n",
    "print(f\"third chunk {recur_chunks[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ba9d8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
